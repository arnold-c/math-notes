\documentclass{article}
\usepackage[margin=1.5cm]{geometry}
\usepackage{microtype} % Slightly better kerning
\usepackage{graphicx}
\graphicspath{ {./figs/} }
\usepackage[colorlinks=true]{hyperref}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\usepackage{fancyhdr} % Needed to adjust page numbering
\usepackage{lastpage} % Counts total number of pages

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % Remove header line
\cfoot{\thepage \hspace{1pt} of \pageref*{LastPage}} % Customize page nums

\usepackage{amsmath}
\usepackage{upgreek}

\usepackage{array} % New env. for equation conditions where aligned
\newenvironment{conditions}[1][where:] 
  {#1 \begin{tabular}[t]{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\\[\belowdisplayskip]}

\usepackage{listings}

\title{Spatial Interpolation Notes}

\begin{document}
\maketitle

\section{Introduction}

We want to use interpolation because it is reasonable to assume that spatially distributed variables are also spatially correlated.
It is not always true, but often worth exploring as part of an analysis.
There are multiple different methods to interpolate data that depend on different underlying assumptions.
These methods are described below.

\begin{center}
    \fbox{Information from \href{https://desktop.arcgis.com/en/arcmap/10.3/tools/3d-analyst-toolbox/how-kriging-works.htm}{ARCGIS} and Applied Spatial Data Analysis With R (2013) unless listed otherwise}
\end{center}

There are two main methods used to interpolate data and estimate a surface for geospatial data:

\begin{itemize}
    \item Inverse distance weighting (IDW) and spline methods
    \item Kriging
\end{itemize}

IDW and splines are deterministic interpolation methods as they are directly based on the surrounding values or smoothed formulas.
Kriging is different as it uses autocorrelation and takes position into account in the statistical models.
Kriging uses a certain number of neighbouring points, or all points within a specified radius (cf kNN).

The general formula for IDW and kriging is:

\[\hat{Z}(s_{0} )=\sum_{i=1}^{N} w(s_{i}) Z (s_{i})\]

\begin{conditions}
Z(s_{i}) & the measured value at the $i$th location \\
w(s_{i}) & an unknown weight for the measured value at the $i$th location \\
s_{0} & the prediction location \\
N & the number of measured values
\end{conditions}

The difference between IDW and kriging is that in IDW, $w(s_{i})$ only depends on distance to prediction location.
In kriging $w(s_{i})$ also depends on autocorrelation i.e. spatial relationship between prediction locations.

\section{Inverse distance weighting (IDW)}

IDW determines cell values using a linearly weighted combination of surrounding values.
The weights are function of the inverse distance.
The general form for the IDW function is:

\[\hat{Z}(s_{0})= \frac{\sum_{i=1}^{n} w(s_{i}) Z(s_{i})}{\sum_{i=1}^{n} w (s_{i})}\]

\begin{conditions}
    Z(s_{i}) & the measured value at the $i$th location \\
    w(s_{i}) & $||s_{i} - s_{0}||^{-p}$ \\
    || \cdot || & Euclidean distance \\
    p & an inverse distance weighting power, defaulting to 2
\end{conditions}

The value of $p$ determines how much closer values are preferred.
As $p$ increases, IDW approaches a one-nearest-neighbour interpolation model.
$p$ can be selected using cross-validation.

Another way to control IDW interpolation is through selecting the number of neighbouring observations to include.
This can improve speed of interpolation, and may be used when there is reason to believe that distant points have little correlation.
There are two approaches for varying the number of points used for interpolation:

\begin{enumerate}
    \item Varying search radius 
    \begin{itemize}
        \item The number of points to include is fixed, and the radius changes to include that set number
        \item Depends on the density of observations fluctuating
        \item The maximum radius can also be set, in which case all points will be included if that max radius is reached before $n$
    \end{itemize}
    \item Fixed search radius
    \begin{itemize}
        \item Set a radius and minimum number of points
        \item If $n <$ minimum number of points at set radius, the radius increases until the minimum is reached.
    \end{itemize}
\end{enumerate}

In addition to these two approaches, barriers can be created to limit the searches for neighbouring points, i.e. only search for this side of a river.

\section{Kriging}

One of the key benefits of kriging is that in addition to using autocorrelation, it is able to estimate uncertainty in the interpolation.
It can do this because it is based on a spatial arrangement of empirical observations, rather than a presumed model of spatial distribution.
Although kriging preferentially weights closer observations, its use of autocorrelation means that clusters are not over-fit i.e. lowering bias as each point in a cluster provides less information than a single point.

The kriging predictor is an "optimal linear predictor" and an exact interpolator.
This means that prediction error is each interpolated value is calculated to minimize the prediction error for that point.
It also means that the interpolated value for sampled points is equal to the actual value, and all interpolated values will be the Best Linear Unbiased Predictors (BLUPs).

Kriging is only helpful where there is at least moderate spatial autocorrelation.
If there is not, then simpler methods like IDW, will generally perform as well as kriging.

\subsection{Assumptions in kriging}

\begin{center}
    \fbox{Information for assumptions from \href{https://www.publichealth.columbia.edu/research/population-health-methods/kriging-interpolation}{Columbia}}    
\end{center}

For kriging to be used, there are a number of assumptions/conditions to be met.
These conditions can be checked in exploratory data analysis.

\begin{enumerate}
    \item Assumption of intrinsic stationarity
    \begin{itemize}
        \item Means that the joint probability distribution does not vary across the study space, so the same parameters (e.g. mean, range and sill etc) are valid across the space
        \item Means one variogram is valid across the space
    \end{itemize}
    \item Assumption of isotropy
    \begin{itemize}
        \item Uniformity in all directions (semivariance identical in all directions)
    \end{itemize}
\end{enumerate}

By making these assumptions, we are assuming that the samples are randomly generated by the function $Z(s)$ with a mean ($m$) and residual ($e(s)$).

\[Z(s) = m + e(s)\]

\begin{conditions}
    E\left(Z(s)\right) & $m$
\end{conditions}

The assumption of \textit{intrinsic stationarity} and \textit{isotropy} can be relaxed to create models where the mean varies spatially.
In instances like this, the measured values can be assumed to be randomly generated by a linear function of known predictors $X_j(s)$.

\begin{equation}
    \begin{split}
        Z(s) & = \sum_{j=0}^p X_j(s)\beta_j + e(s) \\
        & = X\beta + e(s)
    \end{split}    
\end{equation}


\subsection{Creating a prediction map with kriging}

There are two steps:

\begin{enumerate}
    \item Create the variograms and covariance functions to estimate the spatial autocorrelation values that depend on the model of autocorrelation (fitting a model).
    \item Predict the unknown values
\end{enumerate}

\subsubsection{Variography (spatial modelling/structural analysis)}

There are often too many pairs of spatial points to calculate and plot the distance for each pair.
Instead, spatial distances are put into lag bins i.e. all points in the range $40m < h \le 50m$ of point A, and calculate the semivariance.
The semivariance is equal to half the variance of the differences between all possible points spaced a constant distance apart.
Assuming \textit{isotropy} and \textit{intrinsic stationarity}, we can generalise the distances between points and use the distance $||h||$ rather than the vector $\textbf{h}$, i.e. use bins.

\[\hat{\gamma} (\tilde{h}_j) = \frac{1}{2N_h}\sum_{i=1}^{N_h}\left(Z(s_i) - Z(s_i + h)\right)^2, \text{ } \forall h \in \tilde{h}_j\]

\begin{conditions}
    Z(s_{i}) & the measured value at the $i$th location \\
    \hat{\gamma} (\tilde{h}_j) & sample variogram \\
    N_h & sample data points \\
    \tilde{h}_j & distance bins (intervals)
\end{conditions}

Plotting the distance vs semivariance produces an empirical semivariogram.
Closer items should be more similar, therefore lower semivariance.
The opposite is true for further points.

A model is fit to the empirical semivariogram (cf regression).
Different types of models can be fit the the semivariogram, and the optimal model can be selected using metrics like RMSE, MLE, and Bayesian methods:

\begin{itemize}
    \item Spherical (most common)
    \item Circular
    \item Exponential
    \item Gaussian
    \item Linear
\end{itemize}

\begin{figure}[h]
    \centering
    \label{Kriging variogram model shapes}
    \caption{Different types of models used in spatial modelling \href{https://www.publichealth.columbia.edu/research/population-health-methods/kriging-interpolation}{(Poilou 2008)}. a) Linear semi-variogram; (b) spherical semi-variogram; (c) exponential semi-variogram; and (d) power semi-variogram}
    \includegraphics[width=10cm]{semivariogram-models.png}
\end{figure} 

There are a number of key points on the figures:

\begin{itemize}
    \item Range
    \begin{itemize}
        \item The Range is the point at which the semivariance first levels off
        \item Items within the range are autocorrelated (distance matters)
        \item Items outside the range are not autocorrelated (distance no longer changes the semivariance)
    \end{itemize}
    \item Sill 
    \begin{itemize}
        \item The Sill is the height at which the semivariance levels off to
    \end{itemize}
    \item Nugget
    \begin{itemize}
        \item The minimum value of semivariance ($\gamma (h = 0)$)
        \item Theoretically there is no semivariance when $h=0$, but in reality it is present due to measurement error or spatial sources of variation at distances smaller than the sample interval (or both)
    \end{itemize}
    \item Partial Sill
    \begin{itemize}
        \item Amount of semivariance between Sill and the Nugget
    \end{itemize}
\end{itemize}

\subsubsection{Predictions}

Now a model has been fit to the semivariance and autocorrelation can be observed, predictions can be made within the domain.
Kriging differs from IDW as it uses the semivariogram to calculate the weights.
There are a number of methods used in kriging:

\begin{enumerate}
    \item Ordinary kriging
    \begin{itemize}
        \item Assumes the constant mean is unknown
    \end{itemize}
    \item Universal kriging
    \begin{itemize}
        \item Assumes there's a prevailing trend, relaxing the assumption of stationarity for the mean, but maintaining a constant variance
        \item Trend is modelled with polynomial function, and subtracted from observed
        \item Semivariogram is modelled on the residuals to produce autocorrelations
    \end{itemize}
    \item Block kriging
    \begin{itemize}
        \item Estimates averaged values over gridded “blocks” rather than single points
        \item These blocks often have smaller prediction errors than are seen for individual points
    \end{itemize}
    \item Covariate kriging
    \begin{itemize}
        \item Additional observed variables (which are often correlated with each other and the variable of interest) are used to enhance the precision of the interpolation of the variable of interest at each location
    \end{itemize}
    \item Poisson kriging
    \begin{itemize}
        \item Used for incidence counts and disease rates 
    \end{itemize}
\end{enumerate}

\subsection{Limitations}

\begin{center}
    \fbox{Information for limitations from \href{https://www.publichealth.columbia.edu/research/population-health-methods/kriging-interpolation}{Columbia}}    
\end{center}

There are a number of limitations of kriging.

\begin{enumerate}
    \item Since the weights of the kriging interpolator depend on the modeled variogram, kriging is quite sensitive to mis-specification of the variogram model
    \item Similarly, the assumptions of the kriging model (e.g. that of second-order stationarity) may be difficult to meet in the context of many environmental exposures
    \begin{itemize}
        \item Some newer methods (e.g. Bayesian approaches) have thus been developed to try and surmount these obstacles
    \end{itemize}
    \item In general, the accuracy of interpolation by kriging will be limited if the number of sampled observations is small, the data is limited in spatial scope, or the data are in fact not amply spatially correlated
    \begin{itemize}
        \item In these cases, a sample variogram is hard to generate, and methods such as land-use regression may prove preferable to kriging for spatial prediction
    \end{itemize}
\end{enumerate}



\section{Natural Neighbour}

Natural neighbour is a local method that examines samples near the point of interest and evaluates the relative overlap with their areas.
The relative overlaps are then used to create the weights for interpolation.
Because of this, it is also known as "area-stealing" (Sibson) interpolation.
Natural neighbour interpolation therefore does not infer trends that are not already present in the data, and the surface passes through the points, and is smooth in between.

The areas are called Voronoi (Thiessen) polygons.
Voronoi polygons are created by examining the space around points and drawing the boundary so that every place inside the boundary is closest to the polygon's point than any other.
Formally this is written as:

\[R_{k} = \left\{x \in X \mid d(x, P_{k}) < d(x, P_{j}),  \text{ } \forall j \neq k)\right\}\]

\begin{conditions}
    R_{k} & Voronoi polygon of point $k$ \\
    P_{k} & Point $k$ \\ 
    P_{j} & Neighbouring point $j$
\end{conditions}

An example of this can be seen in the figure below.

\begin{figure}[h]
    \centering
    \caption{Natural neighbour method of interpolation}
    \label{Natural neighbour method of interpolation}
    \includegraphics[width=5cm]{natural-neighbour.png}
\end{figure}

\section{Splines}

Splines are a smoothing function that pass through all the input points and attempt to create a smooth surface between them.
As such, it is best for gently varying surfaces e.g. pollution concentrations.
The surface is fit to a specified number of neighbouring input points.
The basic spline is also known as a thin plate interpolation.
There are two conditions that minimum curvature splines must follow:

\begin{enumerate}
    \item The surface must pass through all data points
    \item The surface must have minimum curvature i.e. minimize the cumulative sum of squares of the second derivative terms of the surface at each point
\end{enumerate}

One possible issue with thin plate interpolation is that there may be rapid change in first derivatives around each data point.
Increasing the number of points used for interpolation can help to smooth the surface as the cell is influenced by a greater number of more distant points.
Splines create rectangular regions of equal size, with the same number in the $x-$ and $y-$ directions.
Each region must contain at least 8 points, but different densities resulting from data that is not uniformly distributed can lead to regions containing different numbers of points.

Generally, the spline formula is:

\[S(x, y) = T(x, y) + \sum_{j=1}^{N} \lambda_{j} R(r_{j})\]

\begin{conditions}
    N & total number of points to be used in interpolation \\
    \lambda_{j} & coefficients found by the solution of a system of linear equations \\
    r_{j} & the distance from the point $(x,y)$ to the $j$th point
\end{conditions}

There are two spline types, which define the terms $T(x, y)$ and $R(r_{j})$ differently.

\subsection{Regularized splines}

A regularized spline creates a smooth and gradually changing surface, allowing values outside those observed in the data.

\[T(x, y) = a_{1} + a_{2}x + a_{3}y\]

\begin{conditions}
    a_{i} & coefficients found by the solutions of a system of linear equations
\end{conditions}

and, 

\[R(r) = \frac{1}{2 \pi} \left\{\frac{r^2}{4}\left[\ln \left(\frac{r}{2 \uptau}\right) + c - 1 \right] + \uptau^2\left[K_0 \left(\frac{r}{\uptau}\right) + c + \ln \left(\frac{r}{2 \pi}\right)\right]\right\}\]

\begin{conditions}
    r & the distance between the point and the sample \\
    \uptau^2 & the Weight parameter \\
    K_0 & the modified Bessel function \\
    c & a constant equal to 0.577215
\end{conditions}

In regularized splines, the Weight parameter ($\uptau^2$) specifies the weights attached to the third derivatives terms during minimization.
Larger weights result in smoother surfaces and smooth first-derivative surfaces.
Typical values range between 0 and 0.5.

\subsection{Tension splines}

A tension spline creates a less smooth surface with values more tightly constrained by the sample data range.

\[T(x, y) = a_{1} \]

\begin{conditions}
    a_{1} & a coefficient found by the solutions of a system of linear equations
\end{conditions}

and, 

\[R(r) = - \frac{1}{2 \pi \varphi^2} \left[\ln\left(\frac{r \varphi}{2}\right) + c + K_0 \left(r \varphi\right)\right]\]

\begin{conditions}
    r & the distance between the point and the sample \\
    \varphi^2 & the Weight parameter \\
    K_0 & the modified Bessel function \\
    c & a constant equal to 0.577215
\end{conditions}

The tension method differs from regularized splines as it attaches the Weight parameter ($\varphi^2$) to first-derivative terms, not third-derivative terms.
Larger values of $\varphi^2$  lower the tension and result in a coarser surface as the first-derivative surface is not smooth, passing through all the points.
$\varphi^2 = 0$ results in a basic thin plate surface.
Typical values range between 0 and 10.

\section{Datacamp: Visualizing geospatial data in R}

\begin{itemize}
    \item \textit{ggmap} package very useful for quickly producing static spatial plots
    \begin{itemize}
        \item \textit{ggmap:get\_map(long, lat)} pulls basemap based lat long
        \item \textit{ggmap(*ggmap*, base\_layer = ggplot(df, aes(long, lat))) + geom\_point()} allows you to plot layers over map and retain same aes() e.g. for faceting
    \end{itemize}
    \item Different types of spatial data
    \begin{itemize}
        \item Point data
        \item Line data - assumes points connected by straight lines
        \item Polygon data
        \begin{itemize}
            \item Data associated with enclosed area of points
            \item \textit{ggplot2::geom\_poly()}
        \end{itemize} 
        \item Raster (grid) data
        \begin{itemize}
            \item Regular grid specified by origin and steps in x and y axis, and data is associated with cells in grid
            \item \textit{ggplot2::geom\_tile(aes(fill = *var*))} used to create raster
        \end{itemize}
    \end{itemize}
    \item Polygon data
    \begin{itemize}
        \item Difficult to described
        \begin{itemize}
            \item Order of joining up points matters
            \item Polygons may be broken up e.g. by river therefore needing multiple polygons to describe it
        \end{itemize}
    \end{itemize}
    \item \textit{sp} data structures better that \textit{dataframes} for storing spatial data as don't have to repeat info like groups and order for polygons, and contains information about the coordinate sytem itself, which is useful when working with multiple systems/for sharing
    \item \textit{spdf} is an \textbf{S4} data type
    \begin{itemize}
        \item Useful adaptation of \textit{sp} structure as also contains dataframe
        \item Items are \textbf{slots} that are accessed with the \textit{@} symbol e.g. \textit{spdf@polygon}
        \begin{itemize}
            \item Each \textbf{slot} contains a list that is \textit{another} \textbf{S4} object (e.g. Polygons) (see Fig. \ref{spdf data structure})
            
            \begin{figure}[h]
                \centering
                \caption{\textit{spdf} data structure}
                \label{spdf data structure}
                \includegraphics[width=15cm]{spdf.png}
            \end{figure} 

            \item Can pull information as with normal dataframes using $\$$ symbol e.g. 
            \begin{itemize}
                \item \textit{is\_nz = countries\_spdf\$name == ``New Zealand''}
                \item \textit{nz = countries\_spdf[is\_nz, ]}
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item \textit{tmap} package designed to plot spatial data, rather than requiring dataframe format, like \textit{ggplot2}
    \begin{itemize}
        \item \textit{tm\_shape()} adds basemap
        \item \textit{tm\_raster()} creates choropleth for rasters
        \item \textit{tm\_fill()} creates choropleth for polygons
        \item Can save interactive \textit{leaflet} map using \textit{tmap\_save(filename = *.html)}
    \end{itemize}
    \item \textit{raster} package better to work with raster data than \textit{sp} and \textit{ggplot2}
    \begin{itemize}
        \item Creates an \textbf{S4} object
        \item More efficient as stores data in matrix like format, where each value is associated with a cell in the raster grid
        \begin{itemize}
            \item Multiple matrices act as layers to provide more information
            \begin{itemize}
                \item \textit{Multi-band/multi-layer} rasters
                \item e.g. single band for red/green/blue light to produce colours
            \end{itemize}
            \item Reduces reproducing the same grid
        \end{itemize}
        \item \textit{rasterVis::levelplot()} good for quickly visualizing rasters
    \end{itemize}
    \item \textit{classInt::classIntervals()} useful to bin continuous variables for choropleths
    \item \textit{rgdal::readOGR} used to read in shape files (polygons)
    \item \textit{proj4string()} allows you to define the coordinate reference system (CRS) and projection when no present, or print it where it is present
    \item \textit{rgdal::spTransform()} used to transform CRS
    \begin{itemize}
        \item \textit{tmap} does the transformation automatically
    \end{itemize}
    \item \textit{sp::merge(spdf, df, by.x, by.y)} used to add new information to sp dataframe

    \begin{itemize}
        \item \textit{spdf} have information on \textit{x} number of \textbf{poylgons} (\textit{not observations, like a normal df})
        \item Both \textit{@data} and \textit{@polygon} slots contain an \textit{@ID} slot to match the data to the polygons
        \begin{itemize}
            \item Need to make sure you match them when adding new data otherwise may become unordered
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Datacamp: Spatial analysis with sf and raster in R}

\begin{itemize}
    \item \textit{sf} package new way to read and use spatial information
    \begin{itemize}
        \item \textit{sf::st\_read(``file.sh'')} reads shape files into dataframe with special \textit{geometry} \textbf{list column}
        \begin{itemize}
            \item Means they can be manipulated like standard dfs, e.g. using \textit{dplyr} etc
        \end{itemize}
        \item Also save \textbf{metadata} that can be seen with \textit{head()} call
    \end{itemize}
    \item \textit{raster}
    \begin{itemize}
        \item \textit{raster::brick(``file.tif'')} reads in single band raster data
        \item \textit{raster::raster(``file.tif'')} reads in single band raster data
    \end{itemize}
    \item \textit{sf::st\_area()} calculates area of individual features
    \begin{itemize}
        \item Produces a vector \textbf{with units} in metadata
        \begin{itemize}
            \item Means you have to use \textit{unclass()} function to remove units for calculations e.g. \textit{df \%$>$\% filter(unclass(areas) $>$ 3000)}
        \end{itemize}
    \end{itemize}
    \item \textit{sf::st\_area()} calculates area of individual features
    \item \textit{sf::st\_length()} calculates length of individual
    \item \textit{raster::plotRGB()} used to plot multi-band raster images quickly (if bands correspond to RGB)
    \begin{itemize}
        \item \textit{plot()} creates plots for each band separately
    \end{itemize}
    \item To \textbf{add} CRS information:
    \begin{itemize}
        \item For \textit{sf} data:
        \begin{itemize}
            \item \textit{sf::st\_crs()} shows you the current CRS information
            \begin{itemize}
                \item \textit{\$epsg:} gives numeric code
                \item \textit{\$proj4string:} gives string code
            \end{itemize}
            \item \textit{sf::st\_crs()} can assign CRS
            \begin{itemize}
                \item \textit{sf::str\_crs(sf) $<$- 4236} for EPSG
                \item \textit{sf::str\_crs(sf) $<$- ``+proj=...''} for proj4string
            \end{itemize}
        \end{itemize}
        \item For \textit{raster} data:
        \begin{itemize}
            \item \textit{crs()} shows and assigns current CRS information using proj4string
            \begin{itemize}
                \item \textit{crs() $<$- ``+proj=...''}
            \end{itemize} 
        \end{itemize}
    \end{itemize}
    \item To \textbf{change} CRS information:
    \begin{itemize}
        \item \textit{sf::st\_transform(sf, crs = other\_crs(raster, asText=TRUE))} for vectors (\textit{sf} data)
        \begin{itemize}
            \item Can use either EPSG or proj4string
            \item \textit{asText=TRUE} required to force crs from raster format to text when using a mix of polygons and rasters
        \end{itemize}
        \item \textit{raster::projectRaster()} for rasters
        \begin{itemize}
            \item When specifying CRS with EPSG, must use \textit{projectRaster(raster, crs = ``+init=epsg:32618'')}
            \item Use proj4string as normal (\textit{projectRaster(rater, crs = proj4string)})
        \end{itemize}
    \end{itemize}
    \item \textit{sf::st\_cast(, ``MULTIPOINT'')} used to \textit{cast} polygons in \textit{\$geometry} to bundles of points (MULTIPOINT) to then calculate number of vertices
    \begin{itemize}
        \item \textit{sum(sapply(pts, length))}
    \end{itemize}
    \item \textit{sf::st\_simplify()} used to simplify spatial data by reducing number of vertices so will be processed much faster
    \item \textit{methods::as(sf, Class = ``Spatial'')} converts \textit{sf} object to \textit{sp} object
    \item \textit{sf::st\_as\_sf()} converts \textit{sp} object to \textit{sf} object
    \begin{itemize}
        \item Also used to convert dataframe of coordinates to \textit{sf} object
        \item \textit{sf::st\_as\_sf(pts, coords = c(``lon'', ``lat''), crs = proj4string/EPSG)}
        \begin{itemize}
            \item Longitude must be listed first
        \end{itemize}
    \end{itemize}
    \item \textit{sf::st\_write(sf, ``sf.csv'', layer\_options = ``GEOMETRY=AS\_XY'')} to write \textit{sf} object to csv with coordinate information
    \item \textit{raster::aggregate(raster, fact = factor, fun = function)} used to reduce resolution of raster by a certain factor in each direction (x and y) and function (e.g. taking the mean value)
    \item \textit{raster::reclassify(raster, rcl = reclas\_matrix)} reclassifies values in ranges to a new value (all specified in matrix)
    \item Should use projected CRS when doing analysis as will use common distances like meters vs degrees in unprojected CRS
    \begin{itemize}
        \item Make sure all layers have the same CRS so calculations make sense and are aligned
    \end{itemize}
    \item \textit{sf::st\_buffer(sf, dist = x)} Create a buffer of \textit{x}m around a point/section of a spatial plot
    \begin{itemize}
        \item Useful for identifying objects within that buffer
    \end{itemize}
    \item \textit{sf::st\_centroid(sf)} useful for calculating geographic centre of polygons
    \item Defining regions
    \begin{itemize}
        \item Bounding box
        \begin{itemize}
            \item \textit{sf::st\_bbox(poly)} used to calculate points for bounding box
            \item \textit{sf::st\_make\_grid(poly, n = 1)} used to create a bounding box around the polygons (or centroids if specified etc)
            \begin{itemize}
                \item \textit{n = 1} specifies only want one polygon, rather than multi-row, multi-column grid
                
                \begin{figure}[h]
                    \centering
                    \caption{Bounding box example}
                    \label{bounding box}
                    \includegraphics[width=6cm]{bounding-box.png}
                \end{figure} 

            \end{itemize}
        \end{itemize}
        \item Dissolving features
        \begin{itemize}
            \item \textit{sf::st\_union()} 
            \begin{itemize}
                \item Dissolves polygons into a single polygon
                \item Clusters individual points into a MULTIPOINT geometry
            \end{itemize}
        \end{itemize}
        \item Convex Hull
        \begin{itemize}
            \item Used to create tighter bounding box around points
            \begin{itemize}
                \item Need to cluster individual points (e.g. centroids) into MULTIPOINTS before drawing convex hull
            \end{itemize}
            
            \begin{figure}[h]
                \centering
                \caption{Convex hull example}
                \label{convex hull}
                \includegraphics[width=6cm]{convex-hull.png}
            \end{figure} 
            
            \item Can be created around buffers that overlap
            
            \begin{lstlisting}[language=R]
                # Buffer the beech trees by 3000
                beech_buffer <- st_buffer(beech, dist = 3000)

                # Limit the object to just geometry
                beech_buffers <- st_geometry(beech_buffer)

                # Dissolve the buffers
                beech_buf_union <- st_union(beech_buffers)

                # Plot the dissolved buffers
                plot(beech_buf_union)

                # Create the convex hull
                beech_hull <- st_convex_hull(beech_buf_union)
            \end{lstlisting}

            \begin{figure}[h]
                \centering
                \caption{Overlapping buffers that can be used in a convex hull}
                \label{convex hull buffer}
                \includegraphics[width=6cm]{convex-hull-buffer.png}
            \end{figure} 

        \end{itemize}
    \end{itemize}
    \item Multilayer geoprocessing and relationships
    \begin{itemize}
        \item \textit{sf::st\_join(out\_df, attr\_df)} used to join two dataframes with spatial information when there are not columns that can be used to link them, so need to join based on geographic information
        \begin{itemize}
            \item First object is the df you want as the output (contains most useful information)
            \item Second object is the df that contains the attributes you want added
        \end{itemize}
        \item \textit{sf::st\_intersects(large\_df, small\_df)} shows which geographies intersects
        \begin{itemize}
            \item \textit{sf::st\_intersection(large\_df, small\_df)} clips the resulting list so only those that intersect are shown, and not the rest of the polygons
        \end{itemize}
        \item \textit{sf::st\_contains(large\_df, small\_df)} shows which geographies are completely contained in another polygon
        \item \textit{sf::st\_distance(feat1, feat2)} calculates distances between features
    \end{itemize}
    \item Geoprocessing with rasters
    \begin{itemize}
        \item \textit{raster::mask(raster, mask = poly)} to mask all areas but the polygons of interest
        \item \textit{raster::crop(raster, mask = poly)} to crop the raster to just include the polygons (cf. bounding box) 
        \item\textit{raster::extract(raster, poly, fun = mean)} extracts the values from polygons as a list using a function (e.g. NULL returns all values, and mean returns the means from each polygon)
        \item \textit{raster::overlay(raster1, raster2, fun = func)} allows you to perform functions to rasters e.g. first raster contains elevation, second contains the multiplication values, and the function says to multiple the two rasters together
    \end{itemize}
    \item \textit{ggplot2::geom\_sf()} can create maps of \textit{sf} data
    \begin{itemize}
        \item \textit{aes(fill = var)} used to create choropleths
    \end{itemize}
    \item \textit{tmap::tmap\_arrange(map1, map2, nrow)} allows you to plot multiple maps in a grid
\end{itemize}

\section{Datacamp: Spatial statistics in R}







\section{Datacamp: Interactive maps with leaflet with R}



\end{document}