\documentclass{article}
\usepackage[margin=1.5cm]{geometry}
\usepackage{microtype} % Slightly better kerning
\usepackage{fixltx2e} % Allow text subscript
\usepackage{graphicx}
\graphicspath{ {./figs/} }
\usepackage[colorlinks=true]{hyperref}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\usepackage{fancyhdr} % Needed to adjust page numbering
\usepackage{lastpage} % Counts total number of pages

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % Remove header line
\cfoot{\thepage \hspace{1pt} of \pageref*{LastPage}} % Customize page nums


\usepackage{amsmath}
\usepackage{upgreek}

\title{Forecasting Notes}

\begin{document}
\maketitle

\section{DataCamp: forecasting in R course}

\subsection{Exploring time series}

\begin{itemize}
	\item We can explore time series using the \textit{forecast} package.
	\item We can use the \textit{stats::window()} function to select a segment of a timeseries based on \textit{c(year, period)} (quarter) vs indices
	\item 3 key patters in timeseries
	\begin{itemize}
        \item Trend - long term increase or decrease
        \item Seasonality - regular pattern over fixed period e.g. annual
        \item Cyclical - regular pattern but with no fixed period
    \end{itemize}
    \item \textit{forecast::ggseasonplot()} creates plots by year, rather than continuous
    \item \textit{forecast::ggsubseriesplot()} creates plots by quarter over all years
    \item Lag plots can be used to plot one observation against another (state-space) for autocorrelation
    \begin{itemize}
        \item \textit{forecast::gglagplot()} creates the lag plot
        \item \textit{forecast::ggAcf()} calculates the autocorrelation and creates a plot for each lag
        \item Trends induce positive correlations in the early lags
        \item Seasonality will induce peaks at the seasonal lags
        \item Cyclicity induces peaks at the average cycle length
    \end{itemize}
    \item ``White noise" a purely random time series and is basis of forecasting models
    \begin{itemize}
        \item Can use sampling data of ACF to estimate bounds of significance
        \item Use Ljung-Box test to test a group of autocorrelations together, rather than each separately
        \begin{itemize}
            \item Apply to \textit{diff()} of timeseries
            \item $p < 0.05$ would constitute a ``fail'' as it means there is information in the residuals that hasn't been captured by the model
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Benchmark methods and forecast accuracy}

\begin{itemize}
    \item Run lots of simulations based on statistical model and the mean/median is the ``point forecast"
    \begin{itemize}
        \item Should provide prediction intervals
    \end{itemize}
    \item \textbf{Naive} forecast model is very simple, and provides a baseline for more complicated models, that sometimes don't perform better
    \begin{itemize}
        \item Uses most recent obs as next obs
        \item \textit{forecast::naive()} fits Naive forecast
        \item \textit{forecast::snaive()} fits seasonal Naive forecast
    \end{itemize}
    \item See how good forecast is by testing on data already seen
    \begin{itemize}
        \item Fitted values are forecasts based on all prior values (one-step forecasts)
        \begin{itemize}
            \item When parameters estimated, not really forecasts as all data was used to estimate parameters
        \end{itemize}
        \item Use residuals to evaluate model (\textbf{always check residuals before moving forward with model})
        \begin{itemize}
            \item Residuals should look like (gaussian) white noise if good model
            \item Make 4 assumptions (first two critical, last two convenient)
            \begin{enumerate}
                \item Residuals should be uncorrelated
                    \begin{itemize}
                        \item Otherwise there is information in residuals that should have been captured by forecasting methods
                    \end{itemize}
                \item Residuals have mean zero
                \item Residuals have constant variance
                \item Residuals are normally distributed (required for gaussian white noise vs white noise)
            \end{enumerate}
            \item \textit{forecast::checkresiduals()} plots residuals, autocorrelation, histogram, and performs Ljung-Box test
        \end{itemize}
    \end{itemize}
    \item Forecast errors $\neq$ residuals
    \begin{itemize}
        \item Forecast errors are 
        \begin{itemize}
            \item Errors on \textbf{test} set
            \item Based on multi-step forecasts
        \end{itemize} 
        \item Residuals are 
        \begin{itemize}
            \item Errors on \textbf{training} set
            \item Based on one-step forecasts
        \end{itemize}
    \end{itemize}
    \item Best to use \textbf{Mean Absolute Scaled Error} over \textbf{MAE} or \textbf{MSE} when comparing errors in forecasts on different time series as may have different scales
    \begin{itemize}
        \item $MASE = \frac{MAE}{Q} \text{, where } Q = \text{scaling factor}$
    \end{itemize}
    \item \textit{forecast::accuracy()} computes common metrics used for evaluating both residuals and forecast errors
    \item Cross-validation can be performed in a number of ways
    \begin{itemize}
        \item Multiple one-step forecasts can be made progressively moving forward by one observation each time (rolling origin) and averaging metrics
        \begin{itemize}
            \item Can be applied to multi-step forecasting e.g. two-steps ahead, and three-steps ahead etc
        \end{itemize}
        \item \textit{forecast::tsCV()} can perform cross-validation
        \begin{itemize}
            \item Need to compute own error measures
<<message=FALSE>>=
library(forecast)
library(fpp2)
library(tidyverse)
sq <- function(u){u^2}
for(h in 1:10){
    oil %>% tsCV(forecastfunction = naive, h = h) %>%
    sq() %>% mean(na.rm = TRUE) %>% print()
}
@
        \item Can see how the RMSE is increasing with increasing forecast horizon ($h$)
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Exponential smoothing}

\subsubsection{Simplet exponential smoothing models}
\begin{itemize}
    \item Balance naive and mean forecast models by including all information, but more heavily weighting more recent observations
    \item $\hat{y}_{t+h|t} = \alpha y_t + \alpha(1-\alpha) y_{t-1} + \alpha(1-\alpha)^2 y_{t-2} + \ldots \text{, where } 0 \le \alpha \le 1$
    \begin{itemize}
        \item Describing a function where weights ($\alpha$ terms) decrease exponentially as you go back in time
    \end{itemize}
    \item Equation can be re-written as:
        \begin{itemize}
            \item $\hat{y}_{t+h|t} = \ell_t \text{, where } \ell_t = \alpha y_t + (1 - \alpha)\ell_{t-1}$
            \item $\ell_t$ is known as the ``level" and is the ``smoothing function"
            \begin{itemize}
                \item It is the smoothed value, so updates over time
                \item Need to estimate $\ell_0$, the initial value, and then just update
            \end{itemize}
        \end{itemize}
    \item We choose $\alpha$ and $\ell_0$ to minimize SSE (least squares)
    \begin{itemize}
        \item $SSE = \sum_{t=1}^T \left(y_t - \hat{y}_{t|t-1}\right)^2$
        \begin{itemize}
            \item Have to use this non-linear optimization routine to minimize
        \end{itemize}
    \end{itemize}
    \item \textit{forecast::ses()} function performs simple exponential smoothing
    \item \textit{ggplot2::autolayer(fitted(*ses model*))} useful way of overlaying fitted values as a layer to an \textit{autoplot()} rather than creating a new plot
    \item Only works well when no trend or seasonality
\end{itemize}

\subsubsection{Holt's linear trend model}

\begin{itemize}
    \item Adjusts SES by adding linear trend
    \item Forecast $\hat{y}_{t+h|t} = \ell_t + hb_t$ where:
    \begin{itemize}
        \item $\ell_t = \alpha y_t + (1 - \alpha)(\ell_{t-1} + b_{t-1})$ (``level'')
        \item $b_t = \beta^*(\ell_t - \ell_{t-1}) + (1 - \beta^*)b_{t-1}$ (``trend'')
        \begin{itemize}
            \item $\beta^*$ controls how quickly the slope can change
            \item Because slope can change, often referred to as local linear trend
        \end{itemize}
        \item $0 \le \alpha, \beta^* \le 1$ 
        \item We choose smoothing parameters $\alpha$ and $\beta^*$, and state parameters $\ell_0$ and $b_0$ to minimize SSE (least squares)
    \end{itemize}
    \item A modification can be made to allow the model to ``dampen'' and taper off to a value
    \begin{itemize}
        \item $\hat{y}_{t+h|t} = \ell_t + (\phi + \phi^2 + \ldots + \phi^h)b_t$ (``forecast'')
        \item $\ell_t = \alpha y_t + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})$ (``level'')
        \item $b_t = \beta^*(\ell_t - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}$ (``trend'')
        \item $0 \le \phi \le 1$
        \begin{itemize}
            \item When $\phi = 1$, produces Holt linear trend
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Holt-Winter's model}

\begin{itemize}
    \item Adapted to deal with seasonality
    \item Two versions:
    \begin{enumerate}
        \item Additive
            \begin{itemize}
                \item $\hat{y}_{t+h|t} = \ell_t + hb_t + s_{t-m+h_m^+}$ (``forecast'')
                \item $\ell_t = \alpha (y_t - s_{t-m})+ (1 - \alpha)(\ell_{t-1} + b_{t-1})$ (``level'')
                \item $b_t = \beta^*(\ell_t - \ell_{t-1}) + (1 - \beta^*) b_{t-1}$ (``trend'')
                \item $s_t = \gamma (y_t - \ell_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}$
                \item $s_{t-m+h_m^+}$ is a seasonal component
                \begin{itemize}
                    \item $m$ is the period of seasonality e.g. quarter
                    \item seasonal component averages \textbf{zero}
                \end{itemize}
                \item $0 \le \alpha \le 1, 0 \le \beta^* \le 1, 0 \le \gamma \le 1-\alpha$
            \end{itemize}
        \item Multiplicative
        \begin{itemize}
            \item $\hat{y}_{t+h|t} = \ell_t + hb_t + s_{t-m+h_m^+}$ (``forecast'')
            \item $\ell_t = \alpha \frac{y_t}{ s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})$ (``level'')
            \item $b_t = \beta^*(\ell_t - \ell_{t-1}) + (1 - \beta^*) b_{t-1}$ (``trend'')
            \begin{itemize}
                \item Trend is stil linear
            \end{itemize}
            \item $s_t = \gamma \frac{y_t}{\ell_{t-1} - b_{t-1}} + (1-\gamma)s_{t-m}$
            \begin{itemize}
                \item Seasonality is now multiplicative
            \end{itemize}
            \item $s_{t-m+h_m^+}$ is a seasonal component
            \begin{itemize}
                \item $m$ is the period of seasonality e.g. quarter
                \item seasonal component averages \textbf{one}
            \end{itemize}
            \item $0 \le \alpha \le 1, 0 \le \beta^* \le 1, 0 \le \gamma \le 1-\alpha$
            \item Use multiplicative when seasonal variation increases with the level of the series (as time goes on the smoothed value increases)
        \end{itemize}
    \end{enumerate}
    \item Can add damping to trend of HW models, as with Holt's linear trend models
    \begin{itemize}
        \item Damping can be either additive or multiplicative, however, multiplicative trend damping generally doesn't work well
    \end{itemize}
    \item \textit{forecast::hw()} is used for the Holt-Winter's (and therefore Holt's linear trend) models\
    \begin{itemize}
        \item Set \textit{seasonality = ``additive/multiplicative''}
    \end{itemize}
\end{itemize}

\subsubsection{ Innovations state space models}

\begin{itemize}
    \item The exponential smoothing models discussed are known as \textbf{state space} models
    \begin{itemize}
        \item Each model consists of an equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time
        \item To demonstrate, let's look at an SES model
        \begin{enumerate}
            \item Recall $\hat{y}_{t+h|t} = \ell_t$ and $\ell_t = \alpha y_t + (1-\alpha)\ell_{t-1}$
            \item Rewrite the level function in the ``error correction'' form
            \begin{itemize}
                \item $\ell_t = \alpha y_t + (1-\alpha)\ell_{t-1}$
                \item $\ell_t = \ell_{t-1} + \alpha(y_t - \ell_{t-1})$
                \item $\ell_t = \ell_{t-1} + \alpha e_t$ where:
                \begin{itemize}
                    \item $e_t = y_t - \ell_{t-1}$
                    \item $e_t = y_t - \hat{y}_{t|t-1}$
                    \item $e_t$ is therefore the residual at time $t$
                \end{itemize}
                \item Assuming residuals are normally and independently distributed with mean 0 and variance $\sigma^2$ ($e_t = \upvarepsilon_t \sim NID(0, \sigma^2))$
            \end{itemize}
            \item Rewrite \textit{measurement} and \textit{state} equations
            \begin{itemize}
                \item $y_t = \ell_{t-1} - \upvarepsilon_t$
                \item $\ell_t = \ell_{t-1} + \alpha \upvarepsilon_t$
            \end{itemize}
        \end{enumerate}
    \end{itemize}
    \item Additive and multiplicative models will produce the same ``point forecast'', however, they will differ in their prediction intervals as they will exhibit different errors
    \item We can label state space models as \textbf{ETS} models (Error, Trend, Seasonal) with the following possible labels:
    \begin{itemize}
        \item Error = $\{A, M\}\text{, where } A,M = \text{Additive, Multiplicative}$
        \item Trend = $\{N, A, A_d\} \text{, where } N, A_d = \text{None, Additive damped}$
        \item Seasonal = $\{N, A, M\}$
    \end{itemize}
    \item Multiplicative errors means noise increases with the level of the series (prediction intervals get much wider than additive)
    \item ETS is useful as it allows us to:
    \begin{itemize}
        \item Use MLE to optimize parameters
        \item Generate prediction intervals for all models
        \item Automatically select the best exponential smoothing model for timeseries
        \begin{itemize}
            \item Minimize bias-corrected version of AIC (AIC\textsubscript{c})
            \begin{itemize}
                \item Similar to cross-validation, but much faster
                \item Equivalent to minimizing SSE in models with additive errors
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item \textit{forecasts::ets()} automatically selects ETS model using AIC\textsubscript{c}
    \begin{itemize}
        \item Need to pass to \textit{forecast::forecast()} function for predictions
    \end{itemize}
    \item ETS models are necessarily better than simpler ones e.g. seasonal naive
\end{itemize}

\subsection{Forecasting with ARIMA models}





\end{document}