---
title: "Statistical Rethinking Notes"
subtitle: "2nd Edition"
author: "Callum Arnold"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center")
```


This is a notebook that contains notes and code generated whilst going through
Richard McElreath's Statistical Rethinking book (2nd ed). Each chapter's comments
and answers to the questions will be copied and posted on my website.

> **_NOTE:_** At present the sub-subsection numbers are not correct as I am not
including every sub-subsection. The section and subsection numbers should 
correspond to the correct values in the book

# The Golem of Prague

Chapter one primarily summarizes the motivations behind the book, and a shift
away from traditional frequentist statistical analysis that aim to falsify a 
null hypothesis using a statistical model selected from a flowchart, to a 
Bayesian framework that focuses on model comparison and evaluation, multilevel 
models as the norm, and evaluating the different process models that may link
your statistical model to your hypotheses.

# Small Worlds and Large World

- Can think of models as small worlds vs large worlds:
  - They are at best a small version of reality
  - Often they fail to test/be able to test for a number of errors and possibilities
  - Should always seek to ensure the model performs under favourable assumptions
- Bayesian models are well suited as they are designed to account for this
  - Particularly when frequently tested against reality and updated
  
## The garden of forking data
- Bayesian inferece is primarily just counting and comparing possibilities
  - All possible outcomes (conjectures) are considered, and options "pruned" as 
  more data is considered that do not provide evidence for these outcomes
- Example:
  - Count all the ways the observed data can be observed for each conjecture
  - Consider the relative plausibilities of each conjecture
    - Take counts from the first round, and instead of starting again, use as
    your *prior* for each additional data point
      - Count the number of ways each conjecture can produce the individual data
      point, and multiply this value by the *prior* count
      - Requires that new data is logically independent of previous, otherwise
      recalculate from the start
      - Doesn't require new data to be the same type
    - Convert to probabilities
      - Counts become harder to manage as they increase and we're only interested
      in the relative size of the counts
- Important components of Bayesian formula:
  - Parameter value
    - Conjectured proportion
  - Likelihood
    - Relative number of ways the parameter value can produce the data
  - Prior probability
    - Initial probability of seeing the parameter value
  - Posterior probability
    - Probability of seeing the parameter value after accounting for the new data
    
## Building a model
- Basic process of Bayesian modelling
  1. Determine possible ways data can be observed
    - Often need to describe underlying reality as well as sampling process
    - Be specific with story and resolve ambiguities
  2. Update model by adding data
    - Plausibility of each value of *p*, the estimate of the true value of the 
    parameter, is updated as more data is added
    - Because it it iterative, you can calculate the distributions both forwards
    and backwards given all the data points
    - Means that no minimum sample size is required to be valid (just the 
    inferences will be less clear as wider distribution), but small sample sizes
    are sensitive to priors
  3. Evaluate model and revise
    - Inferences are conditional to the model, so may be incredibly confident
    in model A, but very different with a different model
    - Need to consider the model assumptions, e.g. order of data doesn't matter,
    so that we can consider validity of the model's inferences (which aren't 
    affected by data it doesn't consider)
    - These checks confirm the model's adequacy for a specific purpose

## Components of the model
- Need to explicitly name the variables and provide definitions
- Variables
  - First variable is target of interest (often a proportion)
  - Unobserved variables often called parameters
  - Observed variables are the data
- Definitions
  - Build a model that relates a variable to the others
  - For each parameter, define the probability of seeing each observed variable
  values (data point)
    - Then, for each parameter, define the prior plausibility of each of these 
    values
      - Each specific value of *p* corresponds to a specific plausibility of the
      data
      - We define the plausibility with a distribution (sometimes called the 
      likelihood, but not technically correct in a Bayesian sense)
      - When there are only 2 possible outcomes, we use the binomial distribution
      - For 6 successes in 9 bernoulli trials with prior plausibility 0.5 (*p*), 
      the plausibility = `r dbinom(6, 9, prob = 0.5)`
    - Each parameter must have a prior plausibility
      - This becomes a *prior* when you have previous estimates
      - Can specify and test multiple priors, like with other model components
  - Binomial distribution example
    - $W \sim \text{Binomial}(N, p):  \text{with } p \sim \text{Uniform}(0, 1)$
      - Uniform (flat) prior plausibility
    
## Making the model go
- Model outputs that posterior distribution that is the probability of the 
  parameters conditional on the data (and the model)
- The joint probability of the data $W$ and $L$ with probability $p$ is:

\begin{aligned}
  \text{Pr}(W, L, p) &= \text{Pr}(W, L \mid p)  \times \text{Pr}(p) \\
  &= \text{Pr}(p \mid W, L)  \times \text{Pr}(W, L) \\
  
  &\therefore \\
  \text{Pr}(p \mid W, L)  \times \text{Pr}(W, L) &= \text{Pr}(W, L \mid p)  \times \text{Pr}(p) \\
  \text{Pr}(p \mid W, L) &= \frac{\text{Pr}(W, L \mid p)  \times \text{Pr}(p)}{\text{Pr}(W, L)}
\end{aligned}

- Here $\text{Pr}(W, L)$ is the *average* probability of data (over the prior)
  - AKA the *marginal likelihood*

\begin{aligned}
  \text{Pr}(W, L) &= \text{E}\left(\text{Pr}(W, L \mid p)\right) \\
  &= \int \text{Pr}(W, L \mid p)  \times \text{Pr}(p) dp
\end{aligned}

- Most importantly 
$\text{Pr}(W, L \mid p) \propto  \text{Pr}(W, L \mid p) \times \text{Pr}(p)$
- Often it is not possible to *condition* the prior on the data, so much use
numerical approximations each with different limitations and assumptions
  - Grid approximation
  - Quadratic approximation
  - MCMC
  
### Grid approximation
  
- Converts continuous parameters to finite grids of values
- Limitations
  - Doesn't scale well with increasing number of parameters

Let's have  a look at the binomial example used in the book. Here I turn it into
a function so I can try different priors easily (shown in the book).

```{r}
p_grid <- seq(0, 1, 0.05)

grid_approx <- function(prior){
  # Calculation
  likelihood <- dbinom(6, 9, p_grid)
  unstd_post <- likelihood * prior
  post <- unstd_post / sum(unstd_post)
  
  # Plotting
  plot <- plot(p_grid, post, type = "b")
}
```


```{r}
par(mfrow = c(2, 2))
grid_approx(prior = rep(1, 20))
grid_approx(prior = ifelse(p_grid < 0.5, 0, 1))
grid_approx(prior = exp(-5 * abs(p_grid - 0.5)))
```

### Quadratic approximation

- Assumption that the peak of the posterior distribution can be approximated
by a Guassian distribution
  - First find peak using some optimization algorithm (often based on calculating
  Hessians - the 2nd derivate of the parabola - which is proportional to its std)
  - The log of the Guassian (posterior) is a parabola (i.e. quadratic function)
  - In some instances, is exactly correct
- Computationally inexpensive relative to grid approximation and MCMC
- Limitations
  - Only accurate approximation near the peak of the posterior
  - Relies on decent sample size

We'll use the `quap()` function in the `{rethinking}` package to demonstrate a
binomial example. As in the book, the blue curve is the posterior distribution
calculated numerically using the beta distribution, and the black line is the
quadratic approximation. The key insight is that increasing the amount of data
increases the accuracy of the approximation. This is why frequentist statistics
heavily relies on sample size, as the quadratic approximation is involved. 
However, just because it improves with *n*, doesn't mean high *n* = good model.

> _**NOTE:**_ The Beta distribution is a probability distribution of probabilities
with $\text{Beta}(\alpha, \beta) \text{ where: } \mu = \frac{\alpha}{\alpha \beta}$
see [this answer](https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution)

```{r}
library(rethinking)

quad_approx <- function(W, L){
  globe_qa <- quap(
    alist(
      W ~ dbinom(W+L, prob = p),
      p ~ dunif(0, 1)
    ),
    data = list(W = W, L = L)
  )

  mean <- precis(globe_qa)$mean
  sd <- precis(globe_qa)$sd
  
  curve(dbeta(x, W + 1, L + 1), from = 0, to = 1, col = "blue")
  curve(dnorm(x, mean, sd), add = TRUE)
  mtext(text = glue::glue("n = {W + L}"))
}
```

```{r}
par(mfrow = c(1, 3))
quad_approx(W = 6, L = 3)
quad_approx(W = 18, L = 12)
quad_approx(W = 36, L = 24)
```

