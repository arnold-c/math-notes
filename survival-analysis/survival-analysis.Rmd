---
title: "Survival Analysis Notes"
author: "Callum Arnold"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    number_sections: FALSE
    code_folding: hide
bibliography: survival-analysis.bib
link-citations: TRUE
---

# British Journal of Cancer Tutorial Series {#bjc-tutorials}

## Part 1 [@clarkSurvivalAnalysisPart2003] {#bjc-part1}

-   Often true time to event is not known (event hasn't occurred at end of
    follow up)

-   Survival data rarely normally distributed

    -   Typically many early events and few late ones

### Censoring

-   Survival times unknown for subset of study group

-   Censoring can happen in a number of ways

    -   Patient has not yet experienced outcome by end of study period

        -   **Right censoring**

    -   Patient is lost to follow up during study period

    -   Patient experiences different event making follow up impossible

-   Underestimates the true time to event

-   **Left censored**

    -   If we observe presence of a state/condition but don't know when it
        occurs

    -   We know the start point in this situation e.g. cancer reoccurence
        following surgery, as we know their surgical date

-   **Interval censored**

    -   Observe patients at intervals, and some may be lost to follow up between
        intervals, or time between intervals is large such that a person
        develops disease within the interval and precise onset can't be known

-   Most data is right censored, but methods exist for all types of censoring

### Illustrative Studies

-   Generally good to choose end-point that can't be mis-specified i.e.
    all-cause mortality vs mortality from specific condition

### Survival and Hazard

-   **Survival probability** (*survival function*) $S(t)$

    -   Probability an individual survives from the time origin to specified
        future time *t*

    -   Reflects the cumulative non-occurence

-   **Hazard** $h(t)$ or $\lambda(t)$

    -   Probability that an observed individual has an event at time *t*, i.e.,
        the instantaneous event rate for an individual who has survived to time
        *t*

    -   Hazard relates to the incident event rate

### Kaplan-Meier Survival Estimate

-   Non-parametric estimate from observed survival times

    -   Both censored and non-censored

-   *Assuming independent events*, multiply probabilities of surviving from one
    interval to the next to get the cumulative survival probability

$$
\begin{equation}
\begin{split}
S(t_j) &= S(t_{j-1}) \left(1 - \frac{d_j}{n_j}\right) \\
\text{where}: n_j &= \text{# people alive just before } t_j \\
d_j &= \text{# of events at } t_j
\end{split}
(\#eq:survivalfun)
\end{equation}
$$

-   For censoring, can think of as inputting $d_j = 0$ and reducing $n_j$ by 1
    (assuming one individual was censored at time $t_j$), resulting in
    $S(t_j) = S(t_{j-1})$

-   Often patients lost to follow-up/alive at end of study period, so CIs are
    wider than earlier making for difficulty in interpretation

    -   May want to curtail x-axis for this reason

-   For low incidence events, plotting cumulative incidence instead of
    cumulative survival can be useful

### Hazard and Cumulative Hazard

-   Relationship between survival and hazard given by equation

$$
h(t) = - \frac{d}{dt}[\log S(t)] (\#eq:hazard)
$$

-   No simple way to estimate $h(t)$

    -   Instead need to estimate the **cumulative hazard** $H(t)$

$$
\begin{equation}
\begin{split}
H(t) &= \int h(t) \, dt \\
&= - \log [S(t)]
\end{split}
(\#eq:cum-hazard)
\end{equation}
$$

-   Cumulative hazard can be though of as the number of events to be expected
    for each individual by time $t$ if the event was a repeatable process

-   Cumulative hazard used to estimate $h(t)$, and as a diagnostic tool of model
    validity

-   **Nelson-Aalen estimator** is a simple non-parametric estimate of $H(t)$

-   Can also estimate hazard by assuming survival times follow a specific
    distribution (parametric estimation), for example:

    -   Constant hazard (e.g. healthy individuals) = exponential distribution

    -   Strictly increasing (e.g. leukaemia patients)/decreasing (e.g. patients
        recovery from surgery) hazard = Weibull

    -   Increasing then decreasing log-Normal (e.g. TB patients)

### Non-parametric Tests Comparing Survival

-   Logrank test most widely used to compare curves

    -   Calculates at each event time, for each group, expected number of events
        since previous if no difference between groups, summed over all event
        times, giving total expected number of events for each group

    -   $\chi^2$ test for observed vs expected in each group with $g-1$ degrees
        of freedom where $g$ is the number of groups (survival curves)

    -   When only 2 groups, is $H_0$ that the **hazard ratio** = 1

        -   Usually best to estimate with regression e.g., Cox regression, but
            can calculate with observations/estimated

    -   More robust than comparing medians, but lack of effect size is a
        limitation

-   If natural order, can do a test for trend

    -   Compare to $\chi^2$ distribution with 1 df

### Some Key Requirements for the Analysis of Survival Data

-   Standard methods require **noninformative censoring**

    -   Censored individuals shouldn't differ in their chance of experiencing an
        event e.g. informative when patients withdraw from trial due worsening
        clinical condition

-   Must have sufficient follow up time to capture enough events

    -   Sufficient power for test

-   Good estimate of follow up time is the **reverse KM estimator**

    -   Censoring is the event of interest

-   Unequal of follow up between groups can bias analysis

-   Need to take into account **cohort effects**

    -   Assumption of homogeneity of treatment/other factors during study period
        may not hold e.g. long study period of cancer patients and detection
        methods improve so cases detected earlier in disease progression (not
        accounted for by groups)

-   Consistency in methods between multiple participating centres

## Part 2 [@bradburnSurvivalAnalysisPart2003]

### The Need for Multivariate Statistical Modelling

-   Logrank test only provides p-value, not an estimate of effect size

    -   We need a statistical model to do this, and account for covariates

-   Two main approaches

    -   Proportional hazards e.g. semi-parametric Cox model and fully parametric
        approaches

    -   Accelerated failure time models

### The Cox ("Semi-parametric") Proportional Hazards Model

-   Regression model that describes the relation between event incidence
    (expressed by hazard function) and covariates

$$
h(t) = h_0(t) \times \exp\{b_1 x_1 + b_2 x_2 + ... + b_p x_p \} (\#eq:cox-model)
$$

-   Baseline hazard ($h_0(t)$) estimated non-parametrically, so survival times
    are not assumed to follow a distribution

-   Effectively a linear regression of the logarithm of the hazard on the
    covariates $x_i$ with coefficients $b_i$

    -   Covariates act manipulatively on hazard

-   Key assumption

    -   The hazard in any group is a **constant multiple** of the hazard in the
        other groups i.e. **proportional hazards,** therefore the hazard curves
        cannot cross

        -   Often appropriate assumption for survival data, but must be
            confirmed

-   $\exp(b_i)$ are the **hazard ratios**

    -   HR > 1 means increased hazard of the event i.e. decreased survival time

### Parametric PH Models

-   Same interpretation of regression results as Cox model, but assumes survival
    times and hazards follow a distribution

-   Assumes proportional hazards

-   Most common distributional assumptions

    -   Exponential

    -   Weibull

    -   Gompertz

### Comparison of the Two PH Approaches

-   Sometimes difficult to identify appropriate distribution of survival times

-   Parametric model more informative than Cox model when have correct
    distribution

    -   Straightforward to derive hazard function and obtain predicted survival
        times

    -   Slightly more *efficient* i.e. smaller standard errors

-   For either to be valid:

    -   Covariate effect needs to be at least approximately constant throughout
        study period

    -   Proportionality assumption holds

### Interpreting the PH Model: Beyond the Hazard Ratio

-   Useful to estimate predicted survival proportion at a given time point for a
    group

$$
S(t) = S_0(t) ^{\exp\{b_1 x_1 + b_2 x_2 + ... + b_p x_p \}} (\#eq:surv-prop)
$$

### Accelerated Failure Time Models

$$
\begin{equation}
\begin{split}
S(t) &= S_0 (\varphi t) \\
&= S_0 \left(\exp\{b_1 x_1 + b_2 x_2 + ... + b_p x_p \} t \right)
\end{split}
(\#eq:aft-model)
\end{equation}
$$

-   $S_0(t)$ is the baseline survivor function and $\varphi$ is the
    **accelerator** **factor**
-   The effect of the covariate is to stretch or shrink the survival curve along
    the time axis by a constant relative amount, $\varphi$
-   The AFT model is commonly rewritten as being log-linear with respect to time

$$
\begin{equation}
\begin{split}
\log(T) &= b_1 x_1 + b_2 x_2 + ... + b_p x_p + \varepsilon \\
\text{where}: \varepsilon &= \text{residual variability in survival times}
\end{split}
(\#eq:loglin-aft-model)
\end{equation}
$$

-   Survival times multiplied by a constant effect

-   $\exp(b_i)$ are **time ratios**

    -   \>1 for a covariate = increases the time to event

-   When survival times follow Weibull, AFT and PH models are the same

    -   Interpretation of effect sizes differs - time vs hazard ratios

-   Normal distributional assumptions in AFT models:

    -   Log-Normal

    -   Log-Logistic

    -   Generalised Gamma

    -   Weibull

-   Can implement semi-parametric AFT

    -   Baseline survivor function is estimated non-parametrically

-   Assumptions:

    -   Appropriate distribution

    -   Covariate effects assumed to be constant and multiplicative on the
        timescale i.e. impacts on survival by a constant factor

### Which Model Should We Use: PH or AFT?

-   Initially, which fits the data better?

-   Assuming both fit similarly:

    -   Norm within field i.e. ease of comparability if HRs common vs TRs

    -   Parametric approach of AFT offers more in the way of prediction

    -   TRs arguably more interpretable than HRs

### Other Approaches

-   More straighforward to incorporate covariates using stratified survival
    analysis

    -   Use logrank or similar method to compare within strata, and combined for
        overall comparison adjusted for the strata e.g. stage of disease

    -   No distributional assumptions made and simple interpretation

    -   Only applicable with categorical variables and for few covariates

    -   Does not quantify effect, and only p-value for variable of primary
        interest

    -   Good for exploratory analysis

#### Aalen's additive hazard model

-   Assumes covariates impact additively on baseline hazard, but effects are not
    constrained to be constant

    -   Impact can vary over time

$$
h(t) = h_0(t) + b_1(t) x_1 + b_2(t) x_2 + ... + b_p(t) x_p (\#eq:aalen-additive)
$$

-   Not simple to estimate baseline hazard non-parametrically, so need to use
    cumulative baseline, as well as estimating cumulative additional hazards as
    the regression coefficients

$$
B_i(t) = \int_0^t b_i(u) du (\#eq:cum-add-hazard)
$$

-   Plotting $B_i(t)$ is sometimes called Aalen plots, and the slope can be used
    to infer the relative increase in hazard at time $t$, but not the absolute
    increase ($b_i(t)$)

    -   Can be used to informally assess PH assumption

-   Difficult to interpret and $B_i(t)$ coefficients can change rapidly over
    time so no single effect size estimate

## Part 3 [@bradburnSurvivalAnalysisPart2003a]

### Choice of Covariates

-   Need sufficient *number of events (not participants)* for power, especially
    with variable selection

    -   Suggested at least 10 events for each covariate, otherwise coefficients
        will be biased

        -   Including for interactions!

-   Possible reasons to use a multivariable model:

    -   Single factor under investigation, but other (nuisance) factors exist
        that need to be accounted for

    -   Multiple factors of known relevance under investigation

    -   Multiple factors are potentially associated with survival, possibly with
        additional known factors

        -   Care must be taken to account for multiple testing issues that
            inflate significance

-   Stepwise variable selection is a common *semiautomated* method used

    -   Models may lack clinical significance, and prone to overfitting

-   If used for predicting future survival patterns, need to ensure robust
    effect sizes

    -   Bootstrap sampling common

### Assessing the Adequacy of a Model

-   Need to assess **goodness of fit** i.e., predicted outcomes of a groups
    should be similar to observed

    -   Most methods residual methods difficult to understand due to censoring

        -   Residuals tend to be skewed and need smoothing functions applied for
            interpretation

    -   Overall adequacy can be assess using Cox-Snell residuals

    -   Suggested methods of assessing model fits with residuals:

![](images/Screen%20Shot%202021-09-23%20at%201.57.57%20PM.png)

-   For parametric models, selecting the correct distribution is key

    -   The shape of the hazard is the most distinguishing feature - especially
        useful if known *a priori*

        -   Weibull and Gompertz assume hazard is always increasing or
            decreasing

        -   Log-Logistic used when hazard rises to a peak then decreases, or
            always decreases

            -   Log-Normal and Generalized Gamma preferable when hazard rises to
                a peak then decreases

        -   Exponential assumes constant hazard over time

        -   For all, the shape of the distributions depend on
            **hyperparameters** (**ancillary** **parameter**) estimated from the
            data

        -   Can plot smoothed empirical hazard/cumulative hazard against model
            estimated hazards

        -   Can also plot log(-log(survival)) plots

        -   AIC/BIC can compare model viability

            -   Cox model produces a partial likelihood, so cannot be compared
                to fully parametric models using AIC/BIC

    -   Parametric AFT models are arguably more flexible than Cox model, so may
        also be used when fully parametric models can't

-   Simple test of model adequacy is to compare model survival predictions to KM
    survival curve

-   More formal measure compares observed and expected events in different *risk
    groups*

    -   **Predicted risk (prognostic index - PI)** are calculated for each
        patient, risk groups are constructed by ranking PIs, and a score test is
        applied to the differences between observed and expected events in risk
        groups

$$
PI = b_1 x_1 + b_2 x_2 + ... + b_p x_p (\#eq:prog-index)
$$

-   Predicted survival curves can be misleading when multiple associated
    covariates, especially when multicollinearity present

    -   Fixing all other estimates to mean values ignores these correlations

    -   Use PI test instead

### Assessing Whether PH is Appropriate

-   Empirical hazard function generally not well estimated, so better to use
    **log(-log(survival))** plots

    -   Log of cumulative hazards against log of time for assessing PH
        assumption

        -   Remember $H(t) = -\log[S(t)]$

    -   Should result in parallel lines

        -   May not due to lack of proportionality, *or* missing important
            covariate\\

        -   Either way, suggests inadequate model

    -   Weibull or Exponential parametric models should result in straight lines

-   In Cox PH framework, scaled Schoenfeld residuals test, linear correlation
    test, and time-dependent covariate test most powerful diagnostic tests for
    proportionality

    -   First 2 assess residuals over time

    -   Last assesses whether the covariate coefficients change in time i.e.,
        nonconstant hazard ratio

        -   Appealing as assesses *both* nonproportionality *and* model validity

-   Can sometimes use a time-dependent covariate Cox PH model when PH assumption
    violated

    -   Interactions between covariates and log time

### Assessing Whether An AFT Model is Adequate

-   **Quantile-quantile (Q-Q) plots** of the times of the survival percentiles
    should be a straight line of slope $\varphi$ passing through $(0, 0)$.

    -   Good first examination, but can't account for non-linearity due to
        inappropriate/omitted covariates

-   Time-dependent covariate models can be used

## Part 4 [@clarkSurvivalAnalysisPart2003a]

### Categorising Continuous Variables?

-   Occasionally good reasons to categorise variables, so do the following:

    -   Use predetermined cut-points, preferably with clinical meaning

        -   Don't choose cut-points to minimise p-values

    -   Use 2+ categories to reduce loss of information and allow linearity
        assessment

    -   Ensure groups sizes are adequate (individuals + events)

-   Think if smoothing splines would be more appropriate if nonlinear
    associations

### Multiple Measurements for the Same Covariate

-   Requires **time-dependent (updated) covariate** methods

    -   The value of the covariate changes during the study period e.g.
        non-smoker starts smoking

-   Updated hazard formula in the PH model:

$$
h(t) = h_0(t) \exp[b_1 x_1(t)] (\#eq:time-dep-hazard)
$$

-   Harder to use time-dependent AFT models

-   Can also get **time-dependent coefficients** i.e.
    $h(t) = h_0(t) \exp[b_1(t) x_1]$

    -   The effect of the covariate changes during the study period

-   Requires a large amount of data

-   Have to be confident that the collection process itself isn't dependent on
    clinical progress

### Informative Censoring

-   *Ad hoc* method of sensitivity analysis

    -   Assign varying survival times to censored individuals and examine
        differences

    -   Works best when few censored, but also less bias then

-   Assume all censored individuals follow same treatment e.g. patients lost to
    follow-up in smoking cessation trial all considered to be continuing smokers

-   Competing risks or mixture models can be used when individual experiences
    different defined event which are explicitly modelled

### Missing Covariate Data

-   Multiple imputation methods preferable to alternatives
-   Sensitivity analysis should be conducted to compare imputation results
    against complete case etc

### Variable Selection

-   Start with known prognostic factors and those specifically required for
    study aims
-   Backwards selection is best of the selection strategies as full model is the
    only one with accurate standard errors and p-values
-   Lasso and ridge regression possibilities
-   Use bootstrap methods to compare stability and predictive ability of the
    full model vs reduced model

### Measuring the Predictive Ability, and Model Validation

-   Useful models require both internal and external validity
-   *c-index* used to determine discrimination
-   Calibration can be quantified using an estimate of slope shrinkage
-   Can use training and test data for validation
-   Bootstrapping or LOOCV likely more useful for validation

### Analysis with Unmeasured Factors

-   Inappropriate when vital information is missing e.g. cancer stage

-   Possible that some individuals have shared exposure that is unmeasured e.g.
    environmental exposures for a family

    -   Arises in cluster randomised trials and multicentre trials

    -   Considered to be **multilevel** so need to examine variation within and
        between groups

        -   Use **random effects (frailty)** model to allow covariate effects to
            vary across groups

            -   Can also be considered to apply to the individual relating to
                unmeasured variables, for example

            -   Requires precise knowledge of the frailty distribution, which is
                often not available

### Artificial Neural Networks and Regression Trees

-   ANNs work by having a/multiple latent (hidden) layer(s) between the input
    (covariates) and output (survival probability)

    -   The relative importance of the latent variables determines survival

    -   Can more easily incorporate complex covariate-survival relationships
        than Cox regression

    -   High chance of overfitting

    -   Lack of interpretation

    -   Difficulty handling censored survival times

    -   Usually not better than standard techniques, but often misused

-   CARTs are easily interpretable and rely on fewer distributional assumptions

    -   Particularly useful when interactions

    -   Have to decide threshold to categorise continuous variables and correct
        for multiple testing and overfitting

        -   Same issues as forward selection

    -   Doesn't offer effect size estimates as focuses on discrimination

### Different Types of Events or Repeated Events

-   Competing risks analysis

    -   Seperate time to events can be misleading and KM overestimates
        proportion of subjects experiencing each event

        -   Cumulative incidence method useful to overcome this

            -   Overall event probability at time $t$ is the sum of the
                event-specific probabilities

    -   Generally implemented by entering each patient once per event type

        -   Experiencing a different event = censoring time

-   Multiple events of the same type

    -   Common to use the first event only

        -   Ignores information

    -   **Conditional model**

        -   Time broken up into segments by event i.e. only at risk of event 2
            after you've experienced event 1

        -   Can model starting at:

            -   Beginning of the study

            -   The time of the first event

    -   **Marginal model**

        -   Each event is a separate process i.e. time starts at the beginning
            of the follow-up for each individual

        -   Considered to be at-risk for all events, regardless of number of
            previous events e.g. patient can be at risk of event 4, even if they
            were lost to follow up at the 2nd

    -   **Independent increment model**

        -   Similar to conditional, but doesn't account for the number of
            previous events experienced by a patient

            -   Conditional and marginal preferable for this reason

-   All entered as one patient record per event number

```{=html}
<!-- -->
```
-   All applied within usual Cox or AFT model framework

    -   2 exceptions from usual fitting

        -   Cluster effect used to adjust s.e. due to patients repeated

        -   Stratified analysis (except increment model) by event type
            (competing risks) or number (recurring events)

            -   Interactions between covariates and strata can be assessed

# References {#refs}
